{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "\n",
    "import tf_agents as tfa\n",
    "\n",
    "from kaggle_environments import evaluate, make\n",
    "from kaggle_environments.envs.halite.helpers import *\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 25000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaliteWrapper(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # créer un environnement : où récupérer la config ? la renseigner directement ici ? \n",
    "        self._board_size = 7\n",
    "        self._agent_count = 1\n",
    "        self._max_turns = 20\n",
    "        self._starting_halite = 2500\n",
    "        self.env = make(\"halite\", configuration={'episodeSteps':self._max_turns, 'size':self._board_size, \n",
    "                                                 'startingHalite':self._starting_halite})\n",
    "        self.state = self.env.reset(num_agents=self._agent_count)\n",
    "        \n",
    "        self.obs = self.state[0].observation\n",
    "        self.config = self.env.configuration\n",
    "        self.max_cell_halite = self.config.maxCellHalite\n",
    "        self.board = Board(self.obs, self.config)\n",
    "                \n",
    "        # observation_spec : va dépendre de la \"vision\" que l'on donne au vaisseau du plateau\n",
    "        # action_spec \n",
    "        self._channels = 2\n",
    "        self._observation_spec = tfa.specs.BoundedArraySpec(\n",
    "            shape=(self._channels, self._board_size, self._board_size), \n",
    "            dtype=np.int64, maximum=None, minimum=None)\n",
    "        \n",
    "        self._action_def = {0: None,\n",
    "                            1: ShipAction.NORTH,\n",
    "                            2: ShipAction.EAST,\n",
    "                            3: ShipAction.SOUTH,\n",
    "                            4: ShipAction.WEST,\n",
    "#                             5: ShipAction.CONVERT\n",
    "                           }\n",
    "        self._action_spec = tfa.specs.BoundedArraySpec(shape=(), dtype=np.int32, maximum=len(self._action_def)-1, minimum=0)\n",
    "        \n",
    "        # définir des variables de suivi : compteur de tours, halite au tour précédent ... En fonction des besoins\n",
    "        # ATTENTION : Le turn counter indique ici le numéro du tour qui sera lancé au prochain step, pas le numéro du dernier tour résolu \n",
    "        self.turn_counter = 1\n",
    "        self.previous_cargo_halite = 0 \n",
    "        \n",
    "    def get_observation(self, board):\n",
    "        size = board.configuration.size\n",
    "        me = board.current_player\n",
    "\n",
    "        ships = np.zeros((1, size, size))\n",
    "        ship_cargo = np.zeros((1, size, size))\n",
    "#         bases = np.zeros((1, size, size))\n",
    "\n",
    "        # Possible de diviser par 1000 ou par max_halite afin d'avoir une valeur plus proche de 1\n",
    "        map_halite = np.array(board.observation['halite']).reshape(1, size, size)\n",
    "\n",
    "        for iid, ship in board.ships.items():\n",
    "            # ATTENTION : Logique de me = board.current_player robuste ? \n",
    "            ships[0, ship.position[1], ship.position[0]] = 1 if ship.player_id == me.id else -1\n",
    "            # Idem, possible de diviser le halite en cargo\n",
    "            ship_cargo[0, ship.position[1], ship.position[0]] = ship.halite\n",
    "\n",
    "#         for iid, yard in board.shipyards.items():\n",
    "#             bases[0, yard.position[1], yard.position[0]] = 1 if yard.player_id == me.id else -1\n",
    "\n",
    "        # On pourra jouter ship_cargo et base \n",
    "        observation = np.concatenate([map_halite, ships], axis=0)\n",
    "        observation = np.array(observation, dtype='int64')\n",
    "        return observation\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "        # créer un nouvel environnement \n",
    "        # board \n",
    "        self.env = make(\"halite\", configuration={'episodeSteps':self._max_turns, 'size':self._board_size, \n",
    "                                                 'startingHalite':self._starting_halite})\n",
    "        self.state = self.env.reset(num_agents=self._agent_count)\n",
    "        \n",
    "        self.obs = self.state[0].observation\n",
    "        self.config = self.env.configuration\n",
    "        self.max_cell_halite = self.config.maxCellHalite\n",
    "        self.board = Board(self.obs, self.config)\n",
    "        \n",
    "        # réinitialiser TOUS les compteurs déclarés dans l'init \n",
    "        self.turn_counter = 1\n",
    "        self.previous_cargo_halite = 0 \n",
    "        \n",
    "        # renvoyer le time_step initial \n",
    "        \n",
    "        return_object = ts.restart(self.get_observation(self.board))\n",
    "        \n",
    "#         return_object = ts.restart(np.array(self.state_history, dtype=np.int32))\n",
    "        \n",
    "        return return_object\n",
    "    \n",
    "    def _step(self, action):\n",
    "        \n",
    "        # si le jeu est terminé, renvoyer reset() \n",
    "        if self.turn_counter > self._max_turns:\n",
    "            return self._reset()\n",
    "        \n",
    "        # Prendre l'action et l'appliquer au plateau\n",
    "        action = int(action) # nécessaire ? normalement on a déjà fixé que c'était un int \n",
    "        self.board.ships['0-1'].next_action = self._action_def[action]\n",
    "        self.board = self.board.next()\n",
    "        \n",
    "        # Calculer le reward obtenu par l'action \n",
    "        reward = self.board.ships['0-1'].halite - self.previous_cargo_halite \n",
    "        \n",
    "        # Update all counters \n",
    "        self.turn_counter += 1\n",
    "        self.previous_cargo_halite = self.board.ships['0-1'].halite\n",
    "        \n",
    "        # prendre l'action, la transformer en int et l'appliquer au plateau \n",
    "        # board = board.next() : mettre à jour le plateau \n",
    "        # calculer le reward obtenu par l'action \n",
    "        # retourner la nouvelle observation \n",
    "        #  sous forme de time_step intermédiaire si le tour ne finit pas le jeu\n",
    "        #  sous forme de time_step finale autrement \n",
    "        \n",
    "        # final\n",
    "        if self.turn_counter >= self._max_turns:\n",
    "            return_object = ts.termination(self.get_observation(self.board), reward)\n",
    "            return return_object\n",
    "        else:\n",
    "            return_object = ts.transition(self.get_observation(self.board), reward, discount=1.0)\n",
    "            return return_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour tester le fonctionnement \n",
    "# train_py_env = HaliteWrapper()\n",
    "# print(train_py_env.board)\n",
    "# train_py_env._step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = HaliteWrapper()\n",
    "eval_py_env = HaliteWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a q_network based on the observation and action spec (ie, array shapes)\n",
    "# For now, action will be a single command for a single ship \n",
    "from tf_agents.networks import q_network\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    input_tensor_spec=train_env.observation_spec(),\n",
    "    action_spec=train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.7"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\halite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\halite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 2, 7, 7), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.int64, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000025AEC17D248>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 12219.1796875\n",
      "step = 400: loss = 2637.52001953125\n",
      "step = 600: loss = 8903.76953125\n",
      "step = 800: loss = 19512.857421875\n",
      "step = 1000: loss = 1523.799560546875\n",
      "step = 1000: Average Return = 18.600000381469727\n",
      "step = 1200: loss = 27068.755859375\n",
      "step = 1400: loss = 12343.1279296875\n",
      "step = 1600: loss = 8252.443359375\n",
      "step = 1800: loss = 5531.12451171875\n",
      "step = 2000: loss = 17528.552734375\n",
      "step = 2000: Average Return = 110.19999694824219\n",
      "step = 2200: loss = 3240.265869140625\n",
      "step = 2400: loss = 960.57421875\n",
      "step = 2600: loss = 3259.37109375\n",
      "step = 2800: loss = 991.08447265625\n",
      "step = 3000: loss = 2061.1796875\n",
      "step = 3000: Average Return = 153.1999969482422\n",
      "step = 3200: loss = 5871.13818359375\n",
      "step = 3400: loss = 2917.0703125\n",
      "step = 3600: loss = 11257.2626953125\n",
      "step = 3800: loss = 8867.890625\n",
      "step = 4000: loss = 6780.6669921875\n",
      "step = 4000: Average Return = 15.0\n",
      "step = 4200: loss = 14506.859375\n",
      "step = 4400: loss = 1983.68408203125\n",
      "step = 4600: loss = 8718.822265625\n",
      "step = 4800: loss = 29545.796875\n",
      "step = 5000: loss = 6082.39111328125\n",
      "step = 5000: Average Return = 78.5\n",
      "step = 5200: loss = 28305.25\n",
      "step = 5400: loss = 22922.6015625\n",
      "step = 5600: loss = 4290.83056640625\n",
      "step = 5800: loss = 12524.681640625\n",
      "step = 6000: loss = 4454.48291015625\n",
      "step = 6000: Average Return = 105.19999694824219\n",
      "step = 6200: loss = 17912.4296875\n",
      "step = 6400: loss = 10880.7578125\n",
      "step = 6600: loss = 43504.1328125\n",
      "step = 6800: loss = 3409.237548828125\n",
      "step = 7000: loss = 2299.220947265625\n",
      "step = 7000: Average Return = 57.29999923706055\n",
      "step = 7200: loss = 8462.7353515625\n",
      "step = 7400: loss = 4224.6220703125\n",
      "step = 7600: loss = 8015.3369140625\n",
      "step = 7800: loss = 20982.6484375\n",
      "step = 8000: loss = 14318.294921875\n",
      "step = 8000: Average Return = 22.100000381469727\n",
      "step = 8200: loss = 2174.7587890625\n",
      "step = 8400: loss = 13344.2978515625\n",
      "step = 8600: loss = 5870.5625\n",
      "step = 8800: loss = 2355.28857421875\n",
      "step = 9000: loss = 1159.100341796875\n",
      "step = 9000: Average Return = 49.0\n",
      "step = 9200: loss = 18702.318359375\n",
      "step = 9400: loss = 21324.201171875\n",
      "step = 9600: loss = 4317.66455078125\n",
      "step = 9800: loss = 19974.4140625\n",
      "step = 10000: loss = 15568.98828125\n",
      "step = 10000: Average Return = 109.0999984741211\n",
      "step = 10200: loss = 1468.9810791015625\n",
      "step = 10400: loss = 1423.85302734375\n",
      "step = 10600: loss = 6331.3232421875\n",
      "step = 10800: loss = 1773.0355224609375\n",
      "step = 11000: loss = 6791.64013671875\n",
      "step = 11000: Average Return = 9.300000190734863\n",
      "step = 11200: loss = 46910.546875\n",
      "step = 11400: loss = 21826.544921875\n",
      "step = 11600: loss = 9107.1435546875\n",
      "step = 11800: loss = 6287.9609375\n",
      "step = 12000: loss = 10099.673828125\n",
      "step = 12000: Average Return = 52.099998474121094\n",
      "step = 12200: loss = 4820.171875\n",
      "step = 12400: loss = 6061.9736328125\n",
      "step = 12600: loss = 37908.64453125\n",
      "step = 12800: loss = 8026.2998046875\n",
      "step = 13000: loss = 1208.341796875\n",
      "step = 13000: Average Return = 29.100000381469727\n",
      "step = 13200: loss = 8063.00146484375\n",
      "step = 13400: loss = 12979.7421875\n",
      "step = 13600: loss = 26127.01953125\n",
      "step = 13800: loss = 27188.375\n",
      "step = 14000: loss = 9131.421875\n",
      "step = 14000: Average Return = 77.4000015258789\n",
      "step = 14200: loss = 3879.18017578125\n",
      "step = 14400: loss = 1322.6798095703125\n",
      "step = 14600: loss = 11867.11328125\n",
      "step = 14800: loss = 7289.9462890625\n",
      "step = 15000: loss = 31271.310546875\n",
      "step = 15000: Average Return = 43.599998474121094\n",
      "step = 15200: loss = 1356.5478515625\n",
      "step = 15400: loss = 3371.806640625\n",
      "step = 15600: loss = 2433.64892578125\n",
      "step = 15800: loss = 9716.009765625\n",
      "step = 16000: loss = 5579.841796875\n",
      "step = 16000: Average Return = 62.900001525878906\n",
      "step = 16200: loss = 9996.3603515625\n",
      "step = 16400: loss = 56418.4375\n",
      "step = 16600: loss = 3866.442138671875\n",
      "step = 16800: loss = 5654.0224609375\n",
      "step = 17000: loss = 10339.51171875\n",
      "step = 17000: Average Return = 129.39999389648438\n",
      "step = 17200: loss = 21191.232421875\n",
      "step = 17400: loss = 16750.828125\n",
      "step = 17600: loss = 1085.73681640625\n",
      "step = 17800: loss = 2439.073974609375\n",
      "step = 18000: loss = 12955.376953125\n",
      "step = 18000: Average Return = 70.4000015258789\n",
      "step = 18200: loss = 3884.078125\n",
      "step = 18400: loss = 1596.17724609375\n",
      "step = 18600: loss = 15609.625\n",
      "step = 18800: loss = 30621.72265625\n",
      "step = 19000: loss = 18858.228515625\n",
      "step = 19000: Average Return = 36.5\n",
      "step = 19200: loss = 2607.1171875\n",
      "step = 19400: loss = 612.3408203125\n",
      "step = 19600: loss = 12225.2802734375\n",
      "step = 19800: loss = 4486.7578125\n",
      "step = 20000: loss = 5587.169921875\n",
      "step = 20000: Average Return = 46.0\n",
      "step = 20200: loss = 1745.0245361328125\n",
      "step = 20400: loss = 18886.802734375\n",
      "step = 20600: loss = 3085.076171875\n",
      "step = 20800: loss = 12369.7666015625\n",
      "step = 21000: loss = 9229.0712890625\n",
      "step = 21000: Average Return = 81.0999984741211\n",
      "step = 21200: loss = 6569.36962890625\n",
      "step = 21400: loss = 2771.595703125\n",
      "step = 21600: loss = 2451.08251953125\n",
      "step = 21800: loss = 9111.5\n",
      "step = 22000: loss = 7803.2998046875\n",
      "step = 22000: Average Return = 89.19999694824219\n",
      "step = 22200: loss = 2334.216064453125\n",
      "step = 22400: loss = 3327.2529296875\n",
      "step = 22600: loss = 7836.685546875\n",
      "step = 22800: loss = 16243.2763671875\n",
      "step = 23000: loss = 26185.880859375\n",
      "step = 23000: Average Return = 42.5\n",
      "step = 23200: loss = 4523.31103515625\n",
      "step = 23400: loss = 16460.568359375\n",
      "step = 23600: loss = 2316.404296875\n",
      "step = 23800: loss = 3863.109375\n",
      "step = 24000: loss = 15246.98046875\n",
      "step = 24000: Average Return = 21.799999237060547\n",
      "step = 24200: loss = 1188.27978515625\n",
      "step = 24400: loss = 8554.6328125\n",
      "step = 24600: loss = 2096.752685546875\n",
      "step = 24800: loss = 16813.625\n",
      "step = 25000: loss = 6847.2412109375\n",
      "step = 25000: Average Return = 141.89999389648438\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1050003528594967, 250.0)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not time_step.is_last():\n",
    "    action_step = policy.action(time_step)\n",
    "    time_step = environment.step(action_step.action)\n",
    "    episode_return += time_step.reward\n",
    "total_return += episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=1):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 0 |a2 | 0 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 0 |a2 | 0 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 0 |a1 | 0 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 |a0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 |a0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 |a0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 |a0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 |a0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 |a1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 |a1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 |a1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 |a0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 |a0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 |a0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 |a0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 |a1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 |a1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 2 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 |a1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 2 | 0 | 2 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 2 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 2 |a0 | 2 | 0 | 0 |\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "\n",
      "SOUTH\n",
      "| 1 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
      "| 0 | 0 | 3 | 0 | 3 | 0 | 0 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 2 | 0 | 0 | 1 |\n",
      "| 1 | 0 | 0 | 1 | 0 | 0 | 1 |\n",
      "| 0 | 0 | 3 | 0 | 3 | 0 | 0 |\n",
      "| 1 | 1 | 0 |a0 | 0 | 1 | 1 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_step = eval_env.reset()\n",
    "print(eval_py_env.board)\n",
    "\n",
    "while not time_step.is_last():\n",
    "    action_step = eval_policy.action(time_step)\n",
    "    print(eval_py_env._action_def[int(action_step.action)])\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    print(eval_py_env.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = train_py_env.env.run(['random', 'random'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible de faire un step sur l'environnement Kaggle \n",
    "env.step(actions={'0-1': 'NORTH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour obtenir l'id du joueur actuel\n",
    "obs.player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible de définir un agent qui prend en entrée un board plutot que obs et config\n",
    "# On voit qu'on peut aussi fournir à l'environnement Kaggle l'agent sous forme de fonction plutôt que de fichier \n",
    "\n",
    "@board_agent\n",
    "def move_ships_north_agent(board):\n",
    "    for ship in board.current_player.ships:\n",
    "        ship.next_action = ShipAction.NORTH\n",
    "\n",
    "env.reset(2)\n",
    "env.run([move_ships_north_agent, \"random\"])\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Halite (Python 3.7, Tensorflow 2.3)",
   "language": "python",
   "name": "halite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
