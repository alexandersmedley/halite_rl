{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "\n",
    "import tf_agents as tfa\n",
    "\n",
    "from kaggle_environments import evaluate, make\n",
    "from kaggle_environments.envs.halite.helpers import *\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 10  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaliteWrapper(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # créer un environnement : où récupérer la config ? la renseigner directement ici ? \n",
    "        self._board_size = 7\n",
    "        self._agent_count = 1\n",
    "        self._max_turns = 20\n",
    "        self._starting_halite = 2500\n",
    "        self.env = make(\"halite\", configuration={'episodeSteps':self._max_turns, 'size':self._board_size, \n",
    "                                                 'startingHalite':self._starting_halite})\n",
    "        self.state = self.env.reset(num_agents=self._agent_count)\n",
    "        \n",
    "        self.obs = self.state[0].observation\n",
    "        self.config = self.env.configuration\n",
    "        self.max_cell_halite = self.config.maxCellHalite\n",
    "        self.board = Board(self.obs, self.config)\n",
    "                \n",
    "        # observation_spec : va dépendre de la \"vision\" que l'on donne au vaisseau du plateau\n",
    "        # action_spec \n",
    "#         self._channels = 2\n",
    "        self._observation_spec = tfa.specs.BoundedArraySpec(\n",
    "            shape=self.get_observation(self.board).shape, \n",
    "            dtype=np.int64, maximum=None, minimum=None)\n",
    "        \n",
    "        self._action_def = {0: None,\n",
    "                            1: ShipAction.NORTH,\n",
    "                            2: ShipAction.EAST,\n",
    "                            3: ShipAction.SOUTH,\n",
    "                            4: ShipAction.WEST,\n",
    "#                             5: ShipAction.CONVERT\n",
    "                           }\n",
    "        self._action_spec = tfa.specs.BoundedArraySpec(shape=(), dtype=np.int32, maximum=len(self._action_def)-1, minimum=0)\n",
    "        \n",
    "        # définir des variables de suivi : compteur de tours, halite au tour précédent ... En fonction des besoins\n",
    "        # ATTENTION : Le turn counter indique ici le numéro du tour qui sera lancé au prochain step, pas le numéro du dernier tour résolu \n",
    "        self.turn_counter = 1\n",
    "        self.previous_cargo_halite = 0\n",
    "    \n",
    "    # Ajouter la position du vaisseau comme entrée ? \n",
    "    # Dans la fonction, ne pas utiliser self.board afin de pouvoir y faire appel en dehors de la classe\n",
    "    #  plutôt à mettre dans un helper comme suggéré dans le post Kaggle\n",
    "    def get_observation(self, board):\n",
    "        size = board.configuration.size\n",
    "        me = board.current_player\n",
    "\n",
    "#         ships = np.zeros((1, size, size))\n",
    "#         ship_cargo = np.zeros((1, size, size))\n",
    "#         bases = np.zeros((1, size, size))\n",
    "\n",
    "        # Possible de diviser par 1000 ou par max_halite afin d'avoir une valeur plus proche de 1\n",
    "        halite = np.array(board.observation['halite']).reshape(1, size, size)\n",
    "        halite = np.tile(halite, (3,3))\n",
    "        \n",
    "        # Reduce halite map to ship surroundings \n",
    "        ship = board.ships['0-1']\n",
    "         # convert Halite SDK position format to np matrix \n",
    "        row = size - 1 - ship.position[1] + size\n",
    "        col = ship.position[0] + size\n",
    "        radius = 3 \n",
    "        halite_radius = halite[0, row-radius+1:row+radius, col-radius+1:col+radius]\n",
    "        \n",
    "#         for iid, ship in board.ships.items():\n",
    "#             # ATTENTION : Logique de me = board.current_player robuste ? \n",
    "#             ships[0, ship.position[1], ship.position[0]] = 1 if ship.player_id == me.id else -1\n",
    "#             # Idem, possible de diviser le halite en cargo\n",
    "#             ship_cargo[0, ship.position[1], ship.position[0]] = ship.halite\n",
    "\n",
    "#         for iid, yard in board.shipyards.items():\n",
    "#             bases[0, yard.position[1], yard.position[0]] = 1 if yard.player_id == me.id else -1\n",
    "        \n",
    "        # On pourra ajouter ships, ship_cargo et base \n",
    "        observation = np.concatenate([halite_radius], axis=0)\n",
    "        observation = np.array(observation, dtype='int64')\n",
    "        return observation\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "        # créer un nouvel environnement \n",
    "        # board \n",
    "        self.env = make(\"halite\", configuration={'episodeSteps':self._max_turns, 'size':self._board_size, \n",
    "                                                 'startingHalite':self._starting_halite})\n",
    "        self.state = self.env.reset(num_agents=self._agent_count)\n",
    "        \n",
    "        self.obs = self.state[0].observation\n",
    "        self.config = self.env.configuration\n",
    "        self.max_cell_halite = self.config.maxCellHalite\n",
    "        self.board = Board(self.obs, self.config)\n",
    "        \n",
    "        # réinitialiser TOUS les compteurs déclarés dans l'init \n",
    "        self.turn_counter = 1\n",
    "        self.previous_cargo_halite = 0 \n",
    "        \n",
    "        # renvoyer le time_step initial \n",
    "        \n",
    "        return_object = ts.restart(self.get_observation(self.board))\n",
    "        \n",
    "#         return_object = ts.restart(np.array(self.state_history, dtype=np.int32))\n",
    "        \n",
    "        return return_object\n",
    "    \n",
    "    def _step(self, action):\n",
    "        \n",
    "        # si le jeu est terminé, renvoyer reset() \n",
    "        if self.turn_counter > self._max_turns:\n",
    "            return self._reset()\n",
    "        \n",
    "        # Prendre l'action et l'appliquer au plateau\n",
    "        action = int(action) # nécessaire ? normalement on a déjà fixé que c'était un int \n",
    "        self.board.ships['0-1'].next_action = self._action_def[action]\n",
    "        self.board = self.board.next()\n",
    "        \n",
    "        # Calculer le reward obtenu par l'action \n",
    "        reward = self.board.ships['0-1'].halite - self.previous_cargo_halite \n",
    "        \n",
    "        # Update all counters \n",
    "        self.turn_counter += 1\n",
    "        self.previous_cargo_halite = self.board.ships['0-1'].halite\n",
    "        \n",
    "        # prendre l'action, la transformer en int et l'appliquer au plateau \n",
    "        # board = board.next() : mettre à jour le plateau \n",
    "        # calculer le reward obtenu par l'action \n",
    "        # retourner la nouvelle observation \n",
    "        #  sous forme de time_step intermédiaire si le tour ne finit pas le jeu\n",
    "        #  sous forme de time_step finale autrement \n",
    "        \n",
    "        # final\n",
    "        if self.turn_counter >= self._max_turns:\n",
    "            return_object = ts.termination(self.get_observation(self.board), reward)\n",
    "            return return_object\n",
    "        else:\n",
    "            return_object = ts.transition(self.get_observation(self.board), reward, discount=1.0)\n",
    "            return return_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour tester le fonctionnement \n",
    "# train_py_env = HaliteWrapper()\n",
    "# print(train_py_env.board)\n",
    "# train_py_env._step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = HaliteWrapper()\n",
    "eval_py_env = HaliteWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a q_network based on the observation and action spec (ie, array shapes)\n",
    "# For now, action will be a single command for a single ship \n",
    "from tf_agents.networks import q_network\n",
    "\n",
    "fc_layer_params = (512,512)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    input_tensor_spec=train_env.observation_spec(),\n",
    "    action_spec=train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\halite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\halite\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 5, 5), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.int64, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000001580490B308>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\halite\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\halite\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 11441502.0\n",
      "step = 400: loss = 102957328.0\n",
      "step = 600: loss = 67968080.0\n",
      "step = 800: loss = 49735984.0\n",
      "step = 1000: loss = 10059871.0\n",
      "step = 1000: Average Return = 0.20000000298023224\n",
      "step = 1200: loss = 7197357.0\n",
      "step = 1400: loss = 1267461.75\n",
      "step = 1600: loss = 1181060.75\n",
      "step = 1800: loss = 356858.6875\n",
      "step = 2000: loss = 385472.0\n",
      "step = 2000: Average Return = 2.0\n",
      "step = 2200: loss = 396004.28125\n",
      "step = 2400: loss = 214724.9375\n",
      "step = 2600: loss = 104323.34375\n",
      "step = 2800: loss = 18886.091796875\n",
      "step = 3000: loss = 49315.5078125\n",
      "step = 3000: Average Return = 0.0\n",
      "step = 3200: loss = 110226.6171875\n",
      "step = 3400: loss = 24770.689453125\n",
      "step = 3600: loss = 24035.53515625\n",
      "step = 3800: loss = 28633.802734375\n",
      "step = 4000: loss = 40346.5703125\n",
      "step = 4000: Average Return = 44.79999923706055\n",
      "step = 4200: loss = 17348.2265625\n",
      "step = 4400: loss = 14877.9404296875\n",
      "step = 4600: loss = 18832.1875\n",
      "step = 4800: loss = 3771.502685546875\n",
      "step = 5000: loss = 18579.49609375\n",
      "step = 5000: Average Return = 32.400001525878906\n",
      "step = 5200: loss = 6254.6416015625\n",
      "step = 5400: loss = 5420.1806640625\n",
      "step = 5600: loss = 3374.52783203125\n",
      "step = 5800: loss = 9763.177734375\n",
      "step = 6000: loss = 8125.44873046875\n",
      "step = 6000: Average Return = 73.69999694824219\n",
      "step = 6200: loss = 2741.8330078125\n",
      "step = 6400: loss = 6636.56494140625\n",
      "step = 6600: loss = 6036.0732421875\n",
      "step = 6800: loss = 4409.1240234375\n",
      "step = 7000: loss = 10572.716796875\n",
      "step = 7000: Average Return = 142.6999969482422\n",
      "step = 7200: loss = 3630.1767578125\n",
      "step = 7400: loss = 6400.42333984375\n",
      "step = 7600: loss = 4457.39013671875\n",
      "step = 7800: loss = 5679.1640625\n",
      "step = 8000: loss = 4576.32470703125\n",
      "step = 8000: Average Return = 288.70001220703125\n",
      "step = 8200: loss = 14669.439453125\n",
      "step = 8400: loss = 2964.887939453125\n",
      "step = 8600: loss = 4066.4287109375\n",
      "step = 8800: loss = 3917.079345703125\n",
      "step = 9000: loss = 16602.537109375\n",
      "step = 9000: Average Return = 177.39999389648438\n",
      "step = 9200: loss = 13103.4853515625\n",
      "step = 9400: loss = 13736.7822265625\n",
      "step = 9600: loss = 5575.6142578125\n",
      "step = 9800: loss = 13591.783203125\n",
      "step = 10000: loss = 6964.14111328125\n",
      "step = 10000: Average Return = 122.9000015258789\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-14.435000610351564, 250.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr+klEQVR4nO3deXxV1bn/8c+TGRJmAoRBZkRGxcggHawTFLRa9Vq1rXqdaqvVWm2r/m6He1v7Uq96vbcztbZqnbDVah1wrlobhKDMM0EgzCRAgJD5+f1xdvAEQzhATvZJzvf9ep3X2Wedvfd5VgLnyV5r7bXM3REREamXEnYAIiKSWJQYRESkASUGERFpQIlBREQaUGIQEZEGlBhERKSBuCUGM+tnZm+b2TIzW2JmNwflPzGzjWY2P3hMizrmDjNbbWYrzGxKvGITEZFDs3jdx2BmeUCeu39oZh2AecD5wMXAXne/76D9RwBPAuOB3sAbwDB3r41LgCIi0qi4XTG4+2Z3/zDY3gMsA/o0cch5wFPuXunua4HVRJKEiIi0oLSW+BAzGwCcBHwATAZuNLPLgULgVnffSSRpzI46rJhGEomZXQdcB5CdnX3y8OHD4xu8iIRq5dY9ZKSmMKB79jGdp6qmjhVb99CrYxa5HTKbKbrWad68eTvcPfdQ78c9MZhZDvBX4DvuXmZmvwF+CnjwfD9wFWCNHP6pdi53nwHMAMjPz/fCwsJ4hS4iIdtaVsGEn7/JndOGc93nBh/z+c79xT9JMXj+xs80Q3Stl5mta+r9uI5KMrN0IknhcXd/FsDdt7p7rbvXAb/nk+aiYqBf1OF9gU3xjE9EEtvsohIAJg3q3iznmzY6jwXFu9lQWt4s52ur4jkqyYA/AMvc/YGo8ryo3b4MLA62XwAuMbNMMxsIDAXmxCs+EUl8BWtK6JiVxojeHZvlfNNHR75+Xlm8uVnO11bFsylpMvB1YJGZzQ/K7gQuNbMTiTQTfQx8A8Ddl5jZTGApUAPcoBFJIsmtoKiE8QO7kZrSWEvzkTuuW3tG9enIS4u2NEvTVFsVt8Tg7v+k8X6Dl5s45i7grnjFJCKtx6Zd+1lXUs7lkwY063mnjc7j3lkrKN5ZTt8u7Zv13G2F7nwWkYRUsKa+f6Fbs573QHPSoi3Net62RIlBRBJSQVEJXdqnM7xXh2Y9b/9u2UFzkvoZDkWJQUQSUsGaEiYM7EZKM/UvRJs2Oo/5G3ZRvFOjkxqjxCAiCWdDaTkbd+1n0uDmbUaqp+akpikxiEjCKai/fyFOiaF/t2xG9lZz0qEoMYhIwpm9poRu2RkM7ZETt8+ob07auGt/3D6jtVJiEJGE4u4UFJUwcVA3IvfJxscnzUm6ajiYEoOIJJR1JeVs3l3BxDg1I9Ub0D2bEXlqTmqMEoOIJJQD/QvNfP9CY6aPyeOj9bvYpOakBpQYRCShFKwpIbdDJoNzj22a7VjUNye9rKuGBpQYRCRh1PcvTIpz/0I9NSc1TolBRBLGmu372L6nMm7DVBuj5qRPU2IQkYTRkv0L9aapOelTlBhEJGHMXlNCXqcs+ndruVlPB3bP5oS8jkoMUZQYRCQhuDuzW7B/Idr00b34UM1JBygxiEhCWLVtLyX7quJ+/0Jjph1Y2U1zJ4ESg4gkiHitvxCLQbk5ak6KosQgIgmhYE0JfTq3o1/XcFZVmz66F/PW7WTzbjUnKTGISOjq6pzZa0tadJjqwT4ZnaTmJCUGEQnd8i172FVeHUozUr1BuTkM79VBzUkoMYhIAoj3+guxmj46T81JKDGISAIoWFNC/27t6d25XahxTBujld1AiUFEQlZb53ywtiTUZqR6g9WcBCgxiEjIlm4qY09FTejNSPWmj86jcN1OtuyuCDuU0CgxiEioZocwP1JTDjQnLU7eqwYlBhEJVUFRCYNys+nRMSvsUAA1J4ESg4iEqKa2jjlrSxPmaqHetNF5zP04eZuTlBhEJDSLN5Wxt7KGiQmYGCB5m5OUGEQkNPXzIyVaYhjSI4fjeyZvc5ISg4iEpqCohKE9csjtkBl2KJ8yLRidtLUs+ZqTlBhEJBTVtXUUflyaMMNUDzZ9TC/c4ZUkvGpQYhCRUCws3kV5VW3CdTzXG9KjQ9CclHx3QcctMZhZPzN728yWmdkSM7s5KO9qZq+b2arguUvUMXeY2WozW2FmU+IVm4iEr75/YUKCJgYIRietK2VbkjUnxfOKoQa41d1PACYCN5jZCOB24E13Hwq8GbwmeO8SYCQwFfi1maXGMT4RCVFBUQnDe3Wga3ZG2KEc0oHmpCRb2S1uicHdN7v7h8H2HmAZ0Ac4D3gk2O0R4Pxg+zzgKXevdPe1wGpgfLziE5HwVNbUUvjxzoTtX6g3pEcHhvXM4aWFydXP0CJ9DGY2ADgJ+ADo6e6bIZI8gB7Bbn2ADVGHFQdlB5/rOjMrNLPC7du3xzVuEYmPBRt2U1lTl7D9C9GSsTkp7onBzHKAvwLfcfeypnZtpMw/VeA+w93z3T0/Nze3ucIUkRZUsKYEM5gwMPETw/TReUnXnBTXxGBm6USSwuPu/mxQvNXM8oL384BtQXkx0C/q8L7ApnjGJyLhKCjawYi8jnRqnx52KIc1tGcHhvbI4aUkGrYaz1FJBvwBWObuD0S99QJwRbB9BfB8VPklZpZpZgOBocCceMUnIuGoqK7lw/W7WkUzUr3pY/KY+3HyNCfF84phMvB14HQzmx88pgF3A2eZ2SrgrOA17r4EmAksBWYBN7h7bRzjE5EQfLh+J1U1dQnf8Rytvjlp1pLkaE5Ki9eJ3f2fNN5vAHDGIY65C7grXjGJSPhmrykhxeCUgV3DDiVmB5qTFm7m8kkDwg4n7nTns4i0qIKiEkb36UTHrMTvX4g2bXQecz4uZduett+cpMQgIi1mf1Ut8zfsYmIrakaqN31M0JyUBKOTlBhEpMUUriulutZbVcdzvWE9OzCkR3Lc7KbEICItpmBNCWkpxikDWk//QrRkaU5SYhCRFjO7qIQxfTuRnRm3cS9xVT866dU23pykxCAiLWJfZQ0Li3e3qmGqBxvWMyfSnNTGb3ZTYhCRFjH341Jq6pxJg7qHHcpRM7NIc9LaUrbvqQw7nLhRYhCRFlFQVEJ6qnFy/y6H3zmBTR+dR10bv9lNiUFEWsTsNSWc2K8z7TJa9zIrw3rmMDg3m5fb8OgkJQYRibuyimoWbdzdKoepHszMmD46jw/WlrTZ5iQlBhGJu7lrS6lzWuWNbY2ZNqZtNycpMYhI3BWsKSEjLYVxx7Xu/oV6x/fswKA23JykxCAicVdQVMK44zqTld66+xfqmRnnBM1JW9vgVNxKDCISV7vKq1i6uaxVD1NtzAXj+pKWksKPn1+C+6cWm2zVlBhEJK7mrC3FnVZ9Y1tjBnTP5tazhzFryRaen9+2FptUYhCRuCooKiErPYWx/TqFHUqzu+azgzi5fxd+9PziNtWkpMQgInFVsKaE/P5dyUxrG/0L0VJTjPv+bSxVtXX84K8L20yTkhKDiMRN6b4qlm/Z0+aakaIN7J7NHV88gX+s2M7TczeEHU6zUGIQkbj5oKgEgImDWuc027H6+sT+nDq4Gz99cSkbSsvDDueYKTGISNwUFJXQPiOVMX07hx1KXKWkGPdeNAYz43t/WUBdXetuUlJiEJG4KVhTQv6ArqSntv2vmr5d2vOjc0Ywu6iURwo+DjucY9L2f1siEorteypZtW1vm5gfKVb/lt+X04f34O5XlrNm+96wwzlqSgwiEhezg/6FttzxfDAz4+4LRpOVnsptzyygprYu7JCOihKDiMRFQVEJOZlpjOrdMexQWlSPjln813kj+Wj9Lma8VxR2OEdFiUFE4mL2mhLGD+xKWhL0LxzsS2N7M210Lx58fRXLt5SFHc4RS77fmIjE3dayCop27Euq/oVoZsZPzxtFx3Zp3DpzAVU1ratJKabEYGanmtllZnZ5/SPegYlI65WM/QsH65aTyV1fHs2STWX88u3VYYdzRNIOt4OZPQYMBuYDtUGxA4/GLywRac0K1pTQMSuNE/KSq3/hYFNG9uKCcX341durOfOEHq3mfo7DJgYgHxjhbWUSEBGJu4KiEiYM6kZqioUdSuh+fO5I/rW6hO/OXMCL3/5Mq1iTIpampMVAr3gHIiJtw6Zd+1lXUs7EJO1fOFindunce9EYVm/bywOvrww7nJjEcsXQHVhqZnOAAytfu/uX4haViLRaBWuC/gUlhgM+NyyXyyYcx+/fK+KsET05ZUBizx0VS2L4SbyDEJG2o6CohC7t0xneq0PYoSSUO6edwHurtnPbMwt45ebP0j4jlq/fcDTZlGRmKcCv3P2dgx+HO7GZPWxm28xscVTZT8xso5nNDx7Tot67w8xWm9kKM5tyTLUSkdAUrClhwsBupKh/oYGczDT++6KxrC8t5+5XlocdTpOaTAzuXgcsMLPjjuLcfwKmNlL+P+5+YvB4GcDMRgCXACODY35tZonfQyMiDWwoLWfjrv1JPUy1KRMHdeOqyQN5tGAd/1y1I+xwDimWzuc8YImZvWlmL9Q/DneQu78LlMYYx3nAU+5e6e5rgdXA+BiPFZEEcaB/QYnhkL435XgG5Wbz/b8soKyiOuxwGhVLYvhP4Bzgv4D7ox5H60YzWxg0NXUJyvoA0UsfFQdlItKKzC4qoXtOBkN75IQdSsLKSk/lgYtPZEtZBT/9+9Kww2nUYRNDY/0LsfQxHMJviNwsdyKwmU8STGONkY3eN2Fm15lZoZkVbt++/SjDEJHm5u4H7l8wU/9CU07s15lvnTaEZ+YV88bSrWGH8ymHTQxmtsfMyoJHhZnVmtlRzQrl7lvdvTbou/g9nzQXFQP9onbtC2w6xDlmuHu+u+fn5uYeTRgiEgfrSsrZvLtCw1RjdNMZQxneqwO3P7uInfuqwg6ngViuGDq4e8fgkQVcCPzyaD7MzPKiXn6ZyM1zAC8Al5hZppkNBIYCc47mM0QkHAWaH+mIZKSl8MDFJ7J7fxU/fH7x4Q9oQUc8u6q7/w04/XD7mdmTQAFwvJkVm9nVwL1mtsjMFgJfAG4JzrkEmAksBWYBN7h77SFOLSIJqGBNCbkdMhnUPTvsUFqNEb07cvMZQ3lx4WZeXNhoI0koYplE74KolylE5k467LxJ7n5pI8V/aGL/u4C7DndeEUk89f0Lk9S/cMSu//xgXl+6lR/+bTHjB3alR4essEOK6Yrh3KjHFGAPkeGlIiIAfLRhF9v3VHKqmpGOWFpqCvdffCLlVbXc+exiEmG+0ljuyX7I3d+PLjCzycC2+IQkIq2Ju3PvrOV0y87gnLG9ww6nVRrSI4fvTTmen720jL9+uJGLTu4bajyxXDH8IsYyEUlC76zczuyiUm46Yyg5mYk7/0+iu2ryQMYP7Mp/vrCETbv2hxrLIRODmU0ys1uBXDP7btTjJ4CmqxAR6uqce2at4Liu7bl0/NHMnCP1UlKM+y4aS6073//LwlCblJq6YsgAcog0N3WIepQBF8U/NBFJdH9fuIllm8u49exhZKRpCfljdVy39tw57QT+uXoHf/5gfWhxHPK6L7i7+R0z+5O7rzOzbHff14KxiUgCq6qp477XVjAiryPnjlHfQnP56oTjeHXJFn7+0jI+N7Q7/bu1/PDfWFJ8bzNbCiwDMLOxZvbr+IYlIonuiQ/WsaF0Pz/44nBNsd2MzIx7LhxDWqpx2zMLqK1r+SalWBLDg0SGqZYAuPsC4HNxjElEEtzeyhp+8dZqJg3qxueGdg87nDand+d2/OTckcz9eCd/fH9ti39+TI2C7r7hoCLdlSySxH7/bhEl+6q4/YvDdUNbnFwwrg9njejJva+uYPW2PS362bEkhg1mdirgZpZhZrcRNCuJSPLZsbeSh94rYtroXozt1znscNosM+PnXx5NdkYq3525gJrauhb77FgSw/XADUTWRygmMmX2t+IYk4gksF++tZqKmjpuO/v4sENp83I7ZPKz80ezsHg3v/nHmhb73FhmV93h7l91957u3gP4NvDN+IcmIolmfUk5j3+wjovz+zEoV4vxtITpY/I4d2xv/vfNVSzZtLtFPrOpG9z6mdkMM3vRzK42s/Zmdh+wAujRItGJSEK5//UVpKYY3zlzaNihJJX/+tJIumRncOvMBVTWxL+Lt6krhkeJLJbzC2AUMJtIc9IYd7857pGJSEJZvHE3z8/fxFWTB9KzY/gzgCaTLtkZ3H3BaJZv2cP/vrEq7p/X1MQmXd39J8H2q2a2FTjF3SvjHpWIJJx7X11Bp3bpfOPzg8MOJSmdcUJPLs7vy2/fWcOZI3oy7rgucfusJvsYzKyLmXU1s67AFqB91GsRSRL/WrODd1du58YvDKFTu/Sww0laPzxnBHmd2nHbzAXsr4pfk1JTiaETMC/q0RH4MNgujFtEIpJQ3J17XllO705ZfH1S/7DDSWodstK596IxFO3Yx3+/uiJun9PUXEkD4vapItJqvLJ4CwuKd3PvRWPIStfEymGbPKQ73zptcFxHhWnydBE5pJraOu57dQXDeuZw4bhwF4+RT3x/6vC4nl/z5IrIIc0sLKZoxz6+N2U4qZooL2koMYhIo/ZX1fLgGyvJ79+FM0/QrUvJJKbEYGafMbN/D7ZzzWxgfMMSkbA9/P5atu2p5AeaKC/pHDYxmNmPgR8AdwRF6cCf4xmUiIRr576qyHj5E3pwygCNTk82sVwxfBn4ErAPwN03EVniU0TaqF//YzV7K2v43pT4dnJKYoolMVR5ZFVqBzCzll9nTkRazMZd+3mkYB0XjuvL8b30N2AyiiUxzDSz3wGdzexa4A3g9/ENS0TC8uDrKwG45axhIUciYTnsfQzufp+ZnQWUAccDP3L31+MemYi0uJVb9/DXD4u5avJA+nRuF3Y4EpKYbnALEoGSgUgbd++sFWRnpHHDF4aEHYqEKJZRSXvMrOygxwYze87MBrVEkCISf4Ufl/LGsq1cf9pgumRnhB2OhCiWK4YHiKzL8ARgwCVALyIL9jwMnBav4ESkZbg7d7+ynNwOmfz75AFhhyMhi6Xzeaq7/87d97h7mbvPAKa5+9NA/CYEF5EW8+aybRSu28nNZwylfYamUEt2sSSGOjO72MxSgsfFUe95vAITkZZRW+fc++pyBnbP5iun9As7HEkAsSSGrwJfB7YBW4Ptr5lZO+DGOMYmIi3g2Q+LWbl1L7edfTzpqZo+TWJIDO5e5O7nunt3d88Ntle7+353/+ehjjOzh81sm5ktjirramavm9mq4LlL1Ht3mNlqM1thZlOOvWoicjgV1bX8z+srGdO3E9NG9wo7HEkQsYxKyjKzG8zs18GX/cNm9nAM5/4TMPWgstuBN919KPBm8BozG0GkU3tkcMyvzUwrgojE2Z9nr2PT7gpun6qJ8uQTsVw3PkZkFNIU4B2gL7DncAe5+7tA6UHF5wGPBNuPAOdHlT/l7pXuvhZYDYyPITYROUplFdX88u3VfHZod04d0j3scCSBxJIYhrj7D4F97v4IMB0YfZSf19PdNwMEz/WTvPcBNkTtVxyUfYqZXWdmhWZWuH379qMMQ0R+984adpVX84M4rwYmrU8siaE6eN5lZqOATsCAZo6jsWvYRkc8ufsMd8939/zc3NxmDkMkOWwrq+AP/1zLl8b2ZlSfTmGHIwkmlsQwI+gk/g/gBWApcM9Rft5WM8sDCJ63BeXFQPQ4ub5EbqoTkTh48M1V1NQ6t56tifLk05pMDGaWApS5+053f9fdB7l7D3f/3VF+3gvAFcH2FcDzUeWXmFlmsDrcUGDOUX6GiDShaPtenp67gcsmHEf/bppFXz6tycTg7nUc5b0KZvYkUAAcb2bFZnY1cDdwlpmtAs4KXuPuS4CZRK5GZgE3uHvt0XyuiDTt/tdWkpmWwrdPHxp2KJKgYrn3/XUzuw14mmAVNwB3P3jEUQPufukh3jrjEPvfBdwVQzwicpQWbNjFS4s2c9MZQ8ntkBl2OJKgYkkMVwXPN0SVOaCZVUVaEXfnnlnL6ZqdwbWfHRh2OJLAYlmoR/+CRNqA91bt4F9rSvjxuSPokJUedjiSwGK587m9mf2Hmc0IXg81s3PiH5qINJe6usi02n27tOOyCceFHY4kuFiGq/4RqAJODV4XAz+LW0Qi0uz+vnATSzeXcevZw8hM02wz0rRYEsNgd7+X4EY3d99P4zekiUgCqqqp4/7XVjK8VwfOG9vohAIiDcSSGKqCKbYdwMwGA5VxjUpEms2Tc9azvrScH3xxOCkp+ptODi+WUUk/IXJvQT8zexyYDFwZx5hEpJnsq6zhF2+tYsLArpw2TFPISGxiGZX0mpnNAyYSaUK62d13xD0yETlmD723lh17q5hxuabVltgdNjGY2QvAk8AL7r7vcPuLSGLYsbeSGe+uYerIXow7TsuzS+xi6WO4H/gssNTMnjGzi8wsK85xicgx+uVbq9lfXcttU44POxRpZWJpSnoHeCdYUe104FrgYaBjnGMTkaO0obScxz9Yx8X5/RjSIyfscKSViaXzmWBU0rnAV4BxfLIKm4gkoPtfW0GKGd85U9Nqy5GLpY/haWACkZFJvwL+Ecy6KiIJaOmmMp5fsIlvfG4wvTqp1VeOXCxXDH8ELqufBtvMJpvZZe5+w2GOE5EQ3PvqcjpkpvHNzw8OOxRppQ7b+ezus4DRZnaPmX1MZDqM5fEOTESOXMGaEv6xYjs3fGEIndprojw5Ooe8YjCzYcAlwKVACZH1GMzdv9BCsYnIEdi0az+3PD2fPp3bccWpA8IOR1qxppqSlgPvAee6+2oAM7ulRaISkSOyq7yKyx+ew77KGp755iSy0jVRnhy9ppqSLgS2AG+b2e/N7Aw0eZ5IwqmoruWaRwpZX1LOjMvzGd5LI8nl2BwyMbj7c+7+FWA48A/gFqCnmf3GzM5uofhEpAm1dc5NT37EvPU7+Z+vnMikwd3CDknagFg6n/e5++Pufg7QF5gP3B7vwESkae7Oj55fzGtLt/Ljc0YwfUxe2CFJGxHLlBgHuHupu//O3U+PV0AiEptfvrWaxz9Yz/WfH8yVk7UCrzSfI0oMIpIYZs7dwP2vr+SCk/rwg6maC0malxKDSCvz1vKt3PHcIj43LJd7Lhqj6bSl2SkxiLQiH63fybce/5AReR35zVfHkZ6q/8LS/PSvSqSVWLN9L1f9aS49O2bx8JWnkJ0Z0xyYIkdMiUGkFdhWVsEVD88hxYxHrxpPbofMsEOSNkx/cogkuD0V1Vz5x7mU7qviqesm0r9bdtghSRunKwaRBFZVU8f1f57Hyq17+PVXxzGmb+ewQ5IkoCsGkQRVV+fc9swC3l9dwv3/NpbTju8RdkiSJHTFIJKgfv7yMl5YsIkfTB3OhSf3DTscSSJKDCIJ6KH3injon2u58tQBXP/5QWGHI0lGiUEkwTw/fyM/e2kZ00b34ofnjNANbNLiQuljCFaC2wPUAjXunm9mXYksBjQA+Bi42N13hhGfSFjeX72D255ZwISBXXng4hNJTVFSkJYX5hXDF9z9RHfPD17fDrzp7kOBN9EMrpJkFm/czTcem8fg3BxmXJ6vxXYkNInUlHQe8Eiw/QhwfnihiLSsDaXlXPnHuXTMSuNP/z6eTu20XrOEJ6zE4MBrZjbPzK4Lynq6+2aA4LnRsXlmdp2ZFZpZ4fbt21soXJH4KdlbyeUPz6G6to5Hrx5Pr05ZYYckSS6s+xgmu/smM+sBvG5my2M90N1nADMA8vPzPV4BirSE8qoarnqkkE279vP4NRMY0qND2CGJhHPF4O6bgudtwHPAeGCrmeUBBM/bwohNpKXU1NZx4xMfsah4F7+49CTyB3QNOyQRIITEYGbZZtahfhs4G1gMvABcEex2BfB8S8cmbcvC4l388G+LeWv5Vqpq6sIOpwF3587nFvHW8m389PxRnD2yV9ghiRwQRlNST+C5YGx2GvCEu88ys7nATDO7GlgP/FsIsUkbsXHXfq7601x27K3isdnr6JiVxtkjezF9dB6Th3QnIy3ccRcPvL6SmYXF3HTGUL46oX+osYgcrMUTg7sXAWMbKS8BzmjpeKTtKa+q4ZpHCqmsruPlmz7L1rIKXly4mVeXbOEv84rpmJXGlJG9mDYmj8mDWz5JPDZ7Hb94azWXnNKPW84c2qKfLRILTaInbUpdnXPrzAWs2FLGH648hRG9OzKid0e+MLwHlTWjeH/1Dl5cuJlZi7fwzLxiOrVLZ8rInkwf05tTB3eL+4posxZv4UfPL+aM4T342fmjdFezJCQlBmlTHnxzFa8s3sJ/TD+BLxw0G2lmWiqnD+/J6cN7UllTy3srd/Dyos28vGgLMwuL6dw+nSkjejF9TB6T4pAk5n5cyk1PfcSJ/Trzy8vGkaZlOSVBKTFIm/Hiwk3835uruOjkvlz9mYFN7puZlsqZI3py5oieVFTX8t6qHby0cBMvLdrM04Ub6NI+namjejFtdB6TBnU75i/xlVv3cPWf5tK3czv+cMUptMvQXc2SuMy99d4KkJ+f74WFhWGHIQlg8cbdXPTbfzGydyeeuHYCmWlH98VbUV3Luyu389KizbyxdCv7qmrpmp3BlJG9OGdMHhMGdj3iJLF5934u+PW/qKlznv3mqfTr2v6oYhNpLmY2L2o6ok/RFYO0etvKKrj20UK6ts/gt187+aiTAkBWeipnj+zF2SN7UVFdyzsrt/PSws08P38jT85ZT7fsDKaM6sU5o/MYH0OS2F1ezZUPz2VPRQ1Pf2OikoK0CkoM0qpVVNdy3WPz2FVezV++OYncDpnNdu6s9FSmjOzFlCBJ/GPFNl5atIW/fbSRJz6IJImpoyJ9EhMGdvvUTKgV1bVc+1ghRTv28si/j2dk707NFptIPCkxSKvl7tz57CLmb9jFb782Lq5fvFnpqUwdlcfUUXnsr6pPEpt59sONPP7BerrnZPLFoE9i/MDIHcy3PD2fOWtL+b9LT+LUId3jFptIc1NikFbrd+8W8exHG/nuWcOYOiqvxT63XUYqXxydxxdHR5LE2yu28dLCzTwzbwOPzV5H95xMBuVmM2dtKf8x/QS+NLZ3i8Um0hyUGKRVenPZVu6ZtZxzxuTx7dOHhBZHu4xUpo3OY9roPMqranh7+XZeWrSJt5Zv4/rPD+aaz2pZTml9lBik1Vm5dQ83PfkRo3p34r8vGpswN4m1z0hj+pg8po/Jw90TJi6RI6U7bKRVKd1XxTWPFNI+M40Zl5+csPcDKClIa6YrBmk1qmvr+Nbj89hSVsHT100kr1O7sEMSaZN0xSCtgrvz4xeWMLuolHsuHM1Jx3UJOySRNkuJQVqFx2av44kP1nP95wfz5ZP6hh2OSJumxCAJ7/3VO/jPvy/lzBN68L0px4cdjkibp8QgCW3tjn186/EPGZybzYOXnPSpu4tFpPkpMUjCKquo5ppH5pJi8NDlp5CTqbESIi1B/9MkIdXWOd9+4iPWlZTz52smcFw3TT4n0lKUGCQh3f3KMt5ZuZ2ff3k0Ewd1CzsckaSipiRJOM8UbuD3763likn9uWzCcWGHI5J0lBgkoRR+XMr/e24xnxnSnR+eMyLscESSkhKDJIzineVc/+d59OnSjl9pTWSR0KiPQRLCvsoarn10HpU1dTx1eT6d2qeHHZJI0tKfZBK6ujrn1pkLWLGljF9cehJDeuSEHZJIUlNikNA9+MZKZi3Zwp3TTuC043uEHY5I0lNikFD9fcEm/u+t1Vyc35erPzMw7HBEBCUGCdGi4t3c9swC8vt34afnj9IaBiIJQolBQrGtrIJrHy2ke04mv/36yWSmJeaCOyLJSKOSpMVVVNdy7WPzKKuo5i/Xn0r3nMywQxKRKEoM0qLcnTueXcSCDbv47ddOZkTvjmGHJCIHUVOStKjfvlPEcx9t5NazhjF1VK+wwxGRRiRlYnD3sENISm8s3cq9ry7n3LG9ufH0IWGHIyKHkHBNSWY2FfhfIBV4yN3vbu7PWLKpjAt/8y86tUunY7t0OgWPjllpDco6tkunY1b9dtqB/XIy0xJqBI27U1lTx97KGsora9lXVcO+yhr2VdVGnoNHZU0d7TPT6JCZRk5mGtmZaXTIimznBM+ZaSlxqduKLXu4+amPGNW7E/deOCahfn4i0lBCJQYzSwV+BZwFFANzzewFd1/anJ/TuX06V546gN37q9m9v5qyimq27alg9baaA6+buqhIMRokjejE0TGrYWKJTjj1ZQbsq6qlvP4LvLL2wBd5eVXNgS/4vZU1weuD9j1ou7yqltq65rkKSk81soPEkdMgcaSTk5kalKeTkxUkmKzGk0x2RtqB1dZK91VxzaNzyc5M4/eX59MuQyOQRBJZQiUGYDyw2t2LAMzsKeA8oFkTQ98u7blj2gmHfL+uztlbVcPu8kiS2L2/mrL91ZTtr2mQTOrLd++vZvPu/ZRVRN6vqqlrtljbpaeSnZlGdmYq2RmR567ZGfTr0p7szFTaZ0S+jNsHX9qR15Hy7Mzg2IzIF3dGWkqQYGrZW1HDnspq9lZEEtHeyhr2VEQSzt7KmuD9yPOOvVV8XFJ+oHx/dW1MsWdnRD6/ps7ZW1nDzG9MolenrGb72YhIfCRaYugDbIh6XQxMiN7BzK4DrgM47rj4zNWfkmKRv/yzjm4it4rq2gMJ45ME8klSceeTL+3MtANfoNnRX+aZabRLT232NY5zMtOgw7Gdo6a2jn2VtZHEEjRT7an4JKHsjXpdn2guPLkvJ/br3Cx1EJH4SrTE0Ni3YIM2EnefAcwAyM/PT8he5Kz0VLLSU+nRsW3+dZyWmkKn9imaAVWkjUq0UUnFQL+o132BTSHFIiKSlBItMcwFhprZQDPLAC4BXgg5JhGRpJJQTUnuXmNmNwKvEhmu+rC7Lwk5LBGRpJJQiQHA3V8GXg47DhGRZJVoTUkiIhIyJQYREWlAiUFERBpQYhARkQaUGEREpAElBhERacBa89oEZrYdWHcMp+gO7GimcFqDZKsvqM7JQnU+Mv3dPfdQb7bqxHCszKzQ3fPDjqOlJFt9QXVOFqpz81JTkoiINKDEICIiDSR7YpgRdgAtLNnqC6pzslCdm1FS9zGIiMinJfsVg4iIHESJQUREGkjKxGBmU81shZmtNrPbw47naJlZPzN728yWmdkSM7s5KO9qZq+b2arguUvUMXcE9V5hZlOiyk82s0XBe/9nZs272HQzM7NUM/vIzF4MXrfpOptZZzP7i5ktD37fk5KgzrcE/64Xm9mTZpbV1upsZg+b2TYzWxxV1mx1NLNMM3s6KP/AzAbEFJi7J9WDyAJAa4BBQAawABgRdlxHWZc8YFyw3QFYCYwA7gVuD8pvB+4JtkcE9c0EBgY/h9TgvTnAJCLrbr8CfDHs+h2m7t8FngBeDF636ToDjwDXBNsZQOe2XGegD7AWaBe8nglc2dbqDHwOGAcsjiprtjoC3wJ+G2xfAjwdU1xh/2BC+EVMAl6Nen0HcEfYcTVT3Z4HzgJWAHlBWR6worG6Elkpb1Kwz/Ko8kuB34Vdnybq2Rd4EzidTxJDm60z0DH4krSDyttynfsAG4CuRBYUexE4uy3WGRhwUGJotjrW7xNspxG5U9oOF1MyNiXV/4OrVxyUtWrBJeJJwAdAT3ffDBA89wh2O1Td+wTbB5cnqgeB7wN1UWVtuc6DgO3AH4Pms4fMLJs2XGd33wjcB6wHNgO73f012nCdozRnHQ8c4+41wG6g2+ECSMbE0Fj7Yqses2tmOcBfge+4e1lTuzZS5k2UJxwzOwfY5u7zYj2kkbJWVWcif+mNA37j7icB+4g0MRxKq69z0K5+HpEmk95Atpl9ralDGilrVXWOwdHU8ajqn4yJoRjoF/W6L7AppFiOmZmlE0kKj7v7s0HxVjPLC97PA7YF5Yeqe3GwfXB5IpoMfMnMPgaeAk43sz/TtutcDBS7+wfB678QSRRtuc5nAmvdfbu7VwPPAqfStutcrznreOAYM0sDOgGlhwsgGRPDXGComQ00swwiHTIvhBzTUQlGHvwBWObuD0S99QJwRbB9BZG+h/ryS4KRCgOBocCc4HJ1j5lNDM55edQxCcXd73D3vu4+gMjv7i13/xptu85bgA1mdnxQdAawlDZcZyJNSBPNrH0Q6xnAMtp2nes1Zx2jz3URkf8vh79iCrvjJaTOnmlERvCsAf5f2PEcQz0+Q+SycCEwP3hMI9KG+CawKnjuGnXM/wvqvYKo0RlAPrA4eO+XxNBBFfYDOI1POp/bdJ2BE4HC4Hf9N6BLEtT5P4HlQbyPERmN06bqDDxJpA+lmshf91c3Zx2BLOAZYDWRkUuDYolLU2KIiEgDydiUJCIiTVBiEBGRBpQYRESkASUGERFpQIlBREQaUGKQpGZme4PnAWZ2WTOf+86DXv+rOc8vEi9KDCIRA4AjSgxmlnqYXRokBnc/9QhjEgmFEoNIxN3AZ81sfrAOQKqZ/beZzTWzhWb2DQAzO80ia2A8ASwKyv5mZvOCtQOuC8ruBtoF53s8KKu/OrHg3IuDOfS/EnXuf9gn6y48HjWv/t1mtjSI5b4W/+lIUkkLOwCRBHE7cJu7nwMQfMHvdvdTzCwTeN/MXgv2HQ+Mcve1weur3L3UzNoBc83sr+5+u5nd6O4nNvJZFxC5k3ks0D045t3gvZOAkUTmunkfmGxmS4EvA8Pd3c2sc/NWXaQhXTGINO5s4HIzm09kKvNuROamgcj8NGuj9r3JzBYAs4lMWDaUpn0GeNLda919K/AOcErUuYvdvY7IFCcDgDKgAnjIzC4Ayo+xbiJNUmIQaZwB33b3E4PHQI+sBwCRaa8jO5mdRmQm0EnuPhb4iMj8NIc796FURm3XAmkemUd/PJFZdM8HZh1BPUSOmBKDSMQeIsuj1nsV+GYwrTlmNixYHOdgnYCd7l5uZsOBiVHvVdcff5B3ga8E/Ri5RJZ3nHOowIL1Njq5+8vAd4g0Q4nEjfoYRCIWAjVBk9CfgP8l0ozzYdABvJ3IX+sHmwVcb2YLicx4OTvqvRnAQjP70N2/GlX+HJElGRcQmR33++6+JUgsjekAPG9mWUSuNm45qhqKxEizq4qISANqShIRkQaUGEREpAElBhERaUCJQUREGlBiEBGRBpQYRESkASUGERFp4P8Dy8qIrAw26cEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 3 |a0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "WEST\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a3 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a2 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a1 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a1 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a1 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 3 | 0 | 0 | 0 | 3 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 |a0 | 0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "EAST\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |a0 | 4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "EAST\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 |a4 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 |a3 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 4 | 0 | 0 | 0 | 4 | 1 |\n",
      "\n",
      "None\n",
      "| 1 | 5 | 0 | 0 | 0 | 5 | 1 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 1 |a2 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
      "| 1 | 5 | 0 | 0 | 0 | 5 | 1 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_step = eval_env.reset()\n",
    "print(eval_py_env.board)\n",
    "\n",
    "while not time_step.is_last():\n",
    "    action_step = eval_policy.action(time_step)\n",
    "    print(eval_py_env._action_def[int(action_step.action)])\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    print(eval_py_env.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = train_py_env.env.run(['random', 'random'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible de faire un step sur l'environnement Kaggle \n",
    "env.step(actions={'0-1': 'NORTH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour obtenir l'id du joueur actuel\n",
    "obs.player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible de définir un agent qui prend en entrée un board plutot que obs et config\n",
    "# On voit qu'on peut aussi fournir à l'environnement Kaggle l'agent sous forme de fonction plutôt que de fichier \n",
    "\n",
    "@board_agent\n",
    "def move_ships_north_agent(board):\n",
    "    for ship in board.current_player.ships:\n",
    "        ship.next_action = ShipAction.NORTH\n",
    "\n",
    "env.reset(2)\n",
    "env.run([move_ships_north_agent, \"random\"])\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Halite (Python 3.7, Tensorflow 2.3)",
   "language": "python",
   "name": "halite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
